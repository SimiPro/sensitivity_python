{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np  # Thinly-wrapped version of Numpy\n",
    "from autograd import grad, elementwise_grad\n",
    "from autograd import extend\n",
    "\n",
    "EPS_TOL = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, df_dx, initial_x):\n",
    "    alpha = 1.0\n",
    "    curr_x = initial_x\n",
    "    #   print(\"start val: {}\".format(f(curr_x)))\n",
    "    for i in range(50):\n",
    "        curr_val = f(curr_x)\n",
    "        for j in range(25):\n",
    "            grad_x = df_dx(curr_x)\n",
    "            new_x = curr_x - alpha * grad_x\n",
    "            new_val = f(new_x)\n",
    "            if new_val < curr_val:\n",
    "                curr_x = new_x\n",
    "                break\n",
    "            else:\n",
    "                alpha /= 2.\n",
    "    curr_grad = df_dx(curr_x)\n",
    "    #   print(\"end val: {} | grad_value: {}\".format(f(curr_x), curr_grad))\n",
    "    converged = curr_grad < EPS_TOL\n",
    "    if not converged:\n",
    "        print(\"DID NOT CONVERGE! CHECK\")\n",
    "    return curr_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def O_(x, y):\n",
    "    return (y-2)**2 + 2*x*y\n",
    "\n",
    "@extend.primitive\n",
    "def argmin_O(x, y_init=None, O=O_): # O(x, y) it should have 2 arguments we optimize over y and take derivative w.r.t x\n",
    "    assert y_init is not None\n",
    "    Oopt = lambda y : O(x, y)\n",
    "    return grad_descent(Oopt, grad(Oopt), y_init)\n",
    "\n",
    "\n",
    "def argmin_O_vjp(ans, x, y_init=None, O=O_):\n",
    "    \"\"\"\n",
    "    This should return the jacobian-vector product \n",
    "    it should calculate d_ans/dx because the vector contains dloss/dans\n",
    "    then we get with dloss/dans * dans/dx = dloss/dx which we're actually interested in\n",
    "    \"\"\"\n",
    "    g = grad(O, 1)    \n",
    "    dg_dy = grad(g, 1)(x, y_init)\n",
    "    dg_dx = grad(g, 0)(x, y_init)\n",
    "    \n",
    "    if np.ndim(dg_dy) == 0: # we have just simple scalar function so we just have to divide instead of inverse\n",
    "        return lambda v: v*(-1./dg_dy)*dg_dx\n",
    "    \n",
    "        \n",
    "    return lambda v: v * np.negative(np.matmul(np.linalg.inv(dg_dy), dg_dx))\n",
    "\n",
    "extend.defvjp(argmin_O, argmin_O_vjp)\n",
    "\n",
    "\n",
    "\n",
    "argmin_O(1.0, 5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: \n",
      " 90.0\n",
      "Finite diff grad: 90.00000005698894\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Safety check that gradient is correct\n",
    "\"\"\"\n",
    "from autograd.test_util import check_grads\n",
    "import functools\n",
    "\n",
    "def O1(x, y):\n",
    "    return (2*x - y)**2\n",
    "\n",
    "def example_func(x):\n",
    "    y_init = 7.\n",
    "    y = argmin_O(x, y_init, O1)\n",
    "    return (x - 2*y)**2\n",
    "\n",
    "grad_of_example = grad(example_func)\n",
    "print(\"Gradient: \\n\", grad_of_example(5.))\n",
    "\n",
    "# Check the gradients numerically, just to be safe.\n",
    "\n",
    "def finite_diff(f, x):\n",
    "    h = 1e-7\n",
    "    df_dx = (f(x + h) - f(x -h))/(2.*h)\n",
    "    return df_dx\n",
    "\n",
    "print(\"Finite diff grad: {}\".format(finite_diff(example_func, 5.)))\n",
    "\n",
    "\n",
    "check_grads(example_func, modes=['rev'])(5.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite diff grad: 1.6000000080396148 | Analytical grad: 1.5999999999989738\n"
     ]
    }
   ],
   "source": [
    "def O2(x, y):\n",
    "    return (3*x - 5*y)**2\n",
    "\n",
    "def f2(x):\n",
    "    y_init = 7.\n",
    "    y = argmin_O(x, y_init, O2)\n",
    "    return (x-y)**2\n",
    "\n",
    "print(\"Finite diff grad: {} | Analytical grad: {}\".format(finite_diff(f2, 5.), grad(f2)(5.)))\n",
    "check_grads(f2, modes=['rev'])(5.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
