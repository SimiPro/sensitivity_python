{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np  # Thinly-wrapped version of Numpy\n",
    "from autograd import grad, elementwise_grad\n",
    "from autograd import extend\n",
    "from autograd import test_util\n",
    "EPS_TOL = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, df_dx, initial_x):\n",
    "    alpha = 1.0\n",
    "    curr_x = initial_x\n",
    "    #   print(\"start val: {}\".format(f(curr_x)))\n",
    "    for i in range(25):\n",
    "        curr_val = f(curr_x)\n",
    "        for j in range(25):\n",
    "            grad_x = df_dx(curr_x)\n",
    "            new_x = curr_x - alpha * grad_x\n",
    "            new_val = f(new_x)\n",
    "            if new_val < curr_val:\n",
    "                curr_x = new_x\n",
    "                break\n",
    "            else:\n",
    "                alpha /= 2.\n",
    "    curr_grad = df_dx(curr_x)\n",
    "    #   print(\"end val: {} | grad_value: {}\".format(f(curr_x), curr_grad))\n",
    "    converged = curr_grad < EPS_TOL\n",
    "    if not converged:\n",
    "        print(\"DID NOT CONVERGE! CHECK. grad rest: {}\".format(curr_grad))\n",
    "    return curr_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def O_(x, y):\n",
    "    return (y-2)**2 + 2*x*y\n",
    "\n",
    "@extend.primitive\n",
    "def argmin_O2(x, init=None, O=O_): # O(x, y) it should have 2 arguments we optimize over y and take derivative w.r.t x\n",
    "    assert init is not None\n",
    "    Oopt = lambda y : O(x, y)\n",
    "    return grad_descent(Oopt, grad(Oopt), init)\n",
    "\n",
    "def argmin_O2_vjp(ans, x, init=None, O=O_):\n",
    "    \"\"\"\n",
    "    This should return the jacobian-vector product \n",
    "    it should calculate d_ans/dx because the vector contains dloss/dans\n",
    "    then we get with dloss/dans * dans/dx = dloss/dx which we're actually interested in\n",
    "    \"\"\"\n",
    "    g = grad(O, 1)    \n",
    "    dg_dy = grad(g, 1)(x, init)\n",
    "    dg_dx = grad(g, 0)(x, init)\n",
    "    \n",
    "    if np.ndim(dg_dy) == 0: # we have just simple scalar function so we just have to divide instead of inverse\n",
    "        return lambda v: v*(-1./dg_dy)*dg_dx\n",
    "    \n",
    "        \n",
    "    return lambda v: v * np.negative(np.matmul(np.linalg.inv(dg_dy), dg_dx))\n",
    "\n",
    "extend.defvjp(argmin_O2, argmin_O2_vjp)\n",
    "\n",
    "\n",
    "\n",
    "# in this case O has 3 arguments: O(x, y, z_init) we assume that y = y(x) ? \n",
    "@extend.primitive\n",
    "def argmin_O3(x, y, z_init=None, O=O_):\n",
    "    assert y_init is not None\n",
    "    assert z_init is not None\n",
    "    Oopt = lambda z: O(x, y, z)\n",
    "    return grad_descent(Oopt, grad(Oopt), z_init)\n",
    "\n",
    "def argmin_O3_vjp(ans, x, y, z_init=None, O=O_):\n",
    "    g = grad(O, 2)    \n",
    "    dg_dz = grad(g, 2)(x, y, z_init)\n",
    "    dg_dx = grad(g, 0)(x, y, z_init)\n",
    "    dy_dx = grad(y)(x)\n",
    "    dg_dy = grad(g, 1)(x, y, z_init)\n",
    "    \n",
    "    return lambda v: v*(-(dg_dy + dg_dz*dy_dx))\n",
    "\n",
    "\n",
    "\n",
    "# we always optimize over the last positional arguments in this functions\n",
    "def O2(x, y):\n",
    "    return (3*x - 5*y)**2\n",
    "\n",
    "def O3(x, y, z):\n",
    "    return (x + y - z)**2\n",
    "\n",
    "\n",
    "def O1(x, init_y=3.0, init_z=4.0):\n",
    "    y = argmin_O2(x, init_y, O2)\n",
    "   # z = argmin_O3(x, y, init_z, O3)\n",
    "   # return (x - 2*y)**2 + (z - x)**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical: 0.6400026277324857 | finite diff: 0.6400020857721955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Simple bilevel test\n",
    "\"\"\"\n",
    "\n",
    "def bilevel(x):\n",
    "    y = argmin_O2(x, init=7.0, O=O2)\n",
    "    return (x-y)**2\n",
    "\n",
    "\n",
    "\n",
    "def finite_diff(f, x):\n",
    "    h = 1e-7\n",
    "    return (f(x + h) - f(x - h))/(2.*h)\n",
    "\n",
    "print(\"analytical: {} | finite diff: {}\".format(grad(bilevel)(2.0), finite_diff(bilevel, 2.0)))\n",
    "\n",
    "test_util.check_grads(bilevel, modes=['rev'])(2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical (autodiff): -6.000010934283303 | finite diff: -2.0000003936315736\n",
      "analytical myself: -6.000010934283302\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple trilevel test\n",
    "\"\"\"\n",
    "def O4(y, z):\n",
    "    return (3*y - y*z)**2\n",
    "\n",
    "def simple_trilevel(x):\n",
    "    y = argmin_O2(x, init=7.0, O=O2)\n",
    "    z = argmin_O2(y, init=5.0, O=O4)\n",
    "    return (x -z)**2 # + (y - z)**2\n",
    "\n",
    "\n",
    "def grad_simple_trilevel(x):\n",
    "    y = argmin_O2(x, init=7.0, O=O2)\n",
    "    \n",
    "    z = argmin_O2(y, init=5.0, O=O4)\n",
    "    \n",
    "    OO1 = lambda x,z : (x -z)**2\n",
    "    \n",
    "    dO_dz = grad(OO1, 1)(x, z)\n",
    "    dO_dx = grad(OO1, 0)(x, z)\n",
    "    \n",
    "    dz_dy = grad(argmin_O2)(y, init=5.0, O=O4) #grad(z)\n",
    "    \n",
    "    dy_dx = grad(argmin_O2)(x, init=7.0, O=O2) # grad(y)\n",
    "    # missing term\n",
    "    g2 = grad(O2, 1)\n",
    "    dg2_dz = grad(g2)\n",
    "        \n",
    "    gradi = dO_dx + dO_dz*dz_dy*dy_dx\n",
    "    \n",
    "    return gradi # + (y - z)**2\n",
    "\n",
    "print(\"analytical (autodiff): {} | finite diff: {}\".format(grad(simple_trilevel)(2.0), finite_diff(simple_trilevel, 2.0)))    \n",
    "print(\"analytical myself: {}\".format(grad_simple_trilevel(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
